---
title:  Individual Income Model
date: "`r Sys.Date()`"
author:  Philippe Heymans Smith
output:
  rmdformats::downcute:
    code_folding: show
    fig_width: 8
    fig_height: 4
    use_bookdown: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
static_plots = FALSE
options(scipen = 999)
set.seed(1)
```

```{r helper_functions, include=FALSE}

# fetch the column description from the NAME entries of the data dictionary
describe_column <- function(col = NULL){
  descrip_index <- match(col, dictionaries$names$column_name)
  descr <- dictionaries$names$description[descrip_index]
  descr[is.na(descr)] <- 'Not on ACS dictionary'
}

# quickly format percentage without relying on the {scales} package
fp <- function(x){
  round(x*100,2) %>% paste0('%')
}

# run summary statistics over a numerical column of a data.table (optionally grouped)
num_summary <- function(dt, col, by = NULL){
  # rename the column to a generic variable (to avoid eval() or get() calls)
  data.table::setnames(x = dt, old = col, new = 'value')
  
  # perform the computations
  summ <- dt[, .(count = sum(!is.na(value)),
               distinct_values = uniqueN(value),
               na_count = sum(is.na(value)),
               na_perc = sum(is.na(value))/length(value),
               sum = sum(value, na.rm = TRUE),
               mean = mean(value, na.rm = TRUE),
               sd = sd(value, na.rm = TRUE),
               min = min(value, na.rm = TRUE),
               percentile.1 = quantile(value, 0.1, na.rm = TRUE),
               percentile.2 = quantile(value, 0.2, na.rm = TRUE),
               percentile.3 = quantile(value, 0.3, na.rm = TRUE),
               percentile.4 = quantile(value, 0.4, na.rm = TRUE),
               percentile.5 = quantile(value, 0.5, na.rm = TRUE),
               percentile.6 = quantile(value, 0.6, na.rm = TRUE),
               percentile.7 = quantile(value, 0.7, na.rm = TRUE),
               percentile.8 = quantile(value, 0.8, na.rm = TRUE),
               percentile.9 = quantile(value, 0.9, na.rm = TRUE),
               percentile.95 = quantile(value, 0.95, na.rm = TRUE),
               percentile.975 = quantile(value, 0.975, na.rm = TRUE),
               percentile.99 = quantile(value, 0.99, na.rm = TRUE),
               max = max(value, na.rm = TRUE)),
             by = by]
  
  # rename the column back to its original name
  data.table::setnames(x = dt, old = 'value', new = col)
  
  return(summ)
}

# compute pearson correlation while skipping NAs in the y vector
cor_na.rm <- function(x,y){
  na_index <- is.na(y)
  cor(x[!na_index], y[!na_index])
}


# compute the count and percentage of missing entries for each column of a data.frame 
na_percentage <- function(dt){
  data.table(column = colnames(dt),
             na_count = purrr::map_int(.x = dt, 
                                       .f = ~sum(is.na(.x))
                                       ))[, na_perc := na_count/nrow(dt)][]
}

# removes all columns matching a character vector (optionally regular expressions)
discard_cols <- function(dt, cols, regex = FALSE){
  if(!regex){
    dt %>% purrr::discard(colnames(.) %in% cols)
  }else{
    dt %>% purrr::discard(str_detect(colnames(.), paste(cols, collapse = '|')))
  }
}


na_fill = function(dt, cols, replacement = 0) {
  # match the indexes of the columns specified by the user
  index <- match(cols, colnames(dt))
  purrr::walk(index, ~data.table::set(dt, which(is.na(dt[[.x]])), .x, replacement))
}


# parse the two separate specifications entangled in the ACS dictionary file
parse_dictionaries <- function(dict_filename){
  dictionaries <- readr::read_lines(dict_filename) %>%
    unique() %>%
    split(stringr::str_detect(., '^NAME')) %>% # identify each dataset
    purrr::set_names(c('values', 'names')) %>%
    purrr::map2(.y = list(val_columns = c('field_type',
                                          'column_name',
                                          'column_type',
                                          'field_length',
                                          'starting_value',
                                          'ending_value',
                                          'description'),
                          name_columns = c('field_type',
                                           'column_name',
                                           'column_type',
                                           'field_length',
                                           'description')),
                .f = ~data.table::fread(text = c(paste(.y, collapse = ','),
                                                 .x)))
  
  # find the index of SPORDER in both the $names and $values tables
  sporder_index <- dictionaries %>%
    purrr::map(~which(.x$column_name %chin% 'SPORDER'))
  
  # map the corresponding dataset flag to each row
  walk2(.x = dictionaries,
        .y = sporder_index,
        .f = ~.x[, record_number := 1:.N
        ][, dataset := data.table::fifelse(test = record_number < .y,
                                           yes = 'H',
                                           no = 'P')])
  return(dictionaries)
}

pums_for_individual_modelling <- function(person_filename,
                                         housing_unit_filename,
                                         dictionary_filename){

  tictoc::tic('PUMS files successfully parsed')
  # 0. load and join data ---
    
  # load person data
  per <- data.table::fread('Technical Test Data and Information/psam_p48.csv') %>% 
    discard_cols(cols = 'RT')

  # load housing unit data
  hu <- data.table::fread('Technical Test Data and Information/psam_h48.csv') %>% 
    discard_cols(cols = 'RT')

  dictionaries <- parse_dictionaries(dict_filename)
  
  join_cols <- intersect(colnames(per), colnames(hu))
  # this is c("SERIALNO", "DIVISION", "PUMA", "REGION", "ST", "ADJINC")
  
  # set data.table keys for fast joining
  data.table::setkeyv(per, join_cols)
  data.table::setkeyv(hu,  join_cols)
  
  # data.table left join
  dat <- data.table::merge.data.table(x = per, hu, all.x = TRUE)
  
  # remove the separate tables from memory
  rm(per)
  rm(hu)
  
  # 1. discard columns ----
  
  # weight columns
  dat <- discard_cols(dt = dat, cols = c('^WGTP', '^PWGTP'), regex = TRUE)
  
  # flag columns
  flag_columns <- dictionaries$names[str_detect(description, 'allocation flag')]
  dat <- discard_cols(dt = dat, cols = flag_columns, regex = TRUE)
  
  # constant_columns
  unique_values <- data.table(column = colnames(dat),
                              unique_values = purrr::map_int(dat, uniqueN)
                              )[unique_values == 1]
  
  dat <- discard_cols(dt = dat, cols = unique_values$column)
  
  # 2. data imputation ----
  numeric_impute <- dictionaries$names[column_type == 'N']$column_name %>%
    intersect(colnames(dat)) %>% 
    setdiff('PINCP') # our target variable deserves special consideration 
  
  na_fill(dt = dat,  
          cols = numeric_impute,
          replacement = 0)
  
  # for categoricals, only the variables that have an explicit 'b' = 'Does not apply' field will be replaced.
  categorical_impute <- dictionaries$names[column_type == 'C']$column_name %>%
    intersect(colnames(dat)) %>% 
    intersect(dictionaries$values[column_type == 'C' & starting_value == 'b']$column_name)
  
  na_fill(dt = dat,  
          cols = categorical_impute,
          replacement = 'b')
  
  
  # 3. categorical variable decoding ----
  
  # select the categorical variables according to the dictionary
  columns_to_decode <- dictionaries$names[column_type %chin% 'C']$column_name 
  
  # do not decode range categorical variables (ie where starting_value != ending_value)
  # These are c("SERIALNO", "SERIALNO", "PUMA", "MIGPUMA", "POWPUMA", "RACNUM")
  exclude_from_decoding <- 
    dictionaries$values[column_type %chin% 'C' & starting_value != ending_value]$column_name
  columns_to_decode %<>% 
    setdiff(exclude_from_decoding) 
  
  # and remove the columns we already removed from the dataset
  columns_to_decode %<>% 
    intersect(colnames(dat))
  
  # set all categorical variables as character vectors to avoid type inconsistencies 
  dat[, (columns_to_decode) := purrr::map(.SD, as.character), .SDcols = columns_to_decode]
  
  columns_to_decode %>% walk(decode_categoricals, dat = dat)
  
  tictoc::toc()
  
  return(dat)
}

# decode categorical columns according to the values dictionary
decode_categoricals <- function(col, dat){
  # select the dictionary entries speciying column values
  column_mapping <- dictionaries$values[column_name %chin% col & starting_value == ending_value,
                                        .(starting_value, description)] %>% 
    data.table::setkey(starting_value)
  
  # data.table 'left join' by reference
  # technically this is a mutate by reference, not a join
  matching_statement <- paste0(col, '==', 'starting_value')
  setkeyv(dat, col)
  dat[column_mapping, on = matching_statement, (col) := i.description]
}


# create a new column marking the value band of each observation wrt a continuous column 
discretize_column <- function(dt,
                              amount_column,
                              band_cuts,
                              bands_column_name) {
  
  amount_column %<>% as.character()
  
  # create empty column
  data.table::setDT(dt)
  dt[, (bands_column_name) := NA_character_]
  
  # build index for the bands
  run <- seq_along(band_cuts)
  band_length <- length(band_cuts)
  
  # create names for the bands
  band_names <- paste0(band_cuts[-band_length], '-', band_cuts[-1])
  
  # assign band to each value
  purrr::walk(run, ~data.table::set(dt,
                                    which(band_cuts[.x] <= dt[[amount_column]] &
                                            dt[[amount_column]] <= band_cuts[.x + 1]),
                                    bands_column_name,
                                    band_names[.x])[])
  
  dt[, (bands_column_name) := factor(x = get(bands_column_name), levels = band_names)]
}


```

This document is submitted to Acxiom on behalf of Growth Acceleration Partners for your consideration.

# Problem Statement

The objective set is to predict the individual income (PINCP) of a Texas resident given a sample of the American Community Survey.

The project specifications include two datasets, a data dictionary, and a reference handbook split into 4 chapters and an appendix.

# Data Overview

Both datasets provided are part of the Public Use Microdata Sample (PUMS) initiative by the American Community Survey Office. This initiative is meant to give public access to most questions and answers on the American Community Survey (ACS), while safeguarding the confidentiality of its respondents. Each file is a sample of 1% of its corresponding ACS dataset, which means that larger margins of error should be expected than on ACS-provided summaries.

## Geographic data

The PUMS files contain, in hierarchical order: region, division, state and Public Use Microdata area (PUMA). The latter are non-overlapping areas that cover roughly 100,000 populations each, and this is the lowest level of disaggregation on the file, due to privacy considerations.

# Data processing

We visibly load the libraries that will be used in the study.

```{r libraries, warning=FALSE, message=FALSE}
library(chronicle) # Developed by Philippe Heymans, see its Appendix section for further details 
# remotes::install_github(repo = 'pheymanss/chronicle')
library(data.table)
library(knitr)
library(ggplot2)
library(magrittr)
library(purrr)
library(stringr)
library(readr)
library(skimr)
```

## Data dictionary parsing

To get a better understanding of the data, we proceed to map the PUMS variable names to their corresponding descriptions. The parsing of the data dictionary was rather troublesome, since the ACS houses two record types carelessly braided into the same .csv file:

-   The specification for each field, which comprises 5 values:

1.  Record type (NAME or VAL).
2.  Column name.
3.  Column type (numeric or categorical).
4.  Field length.
5.  Description.

-   The specification for each possible field value, which comprises the same values, but additionally the range of values that each entry can take.

1.  Record type (NAME or VAL).
2.  Column name.
3.  Column type (numeric or categorical).
4.  Field length.
5.  Starting Value (first element of the value range).
6.  Ending Value (last element of the value range).
7.  Description.

This makes the VAL record type a more complete mapping of the content of the data, but the NAME record type is easier to quickly check. We parse them both separately into a list.

```{r dictionary_parsing}
dict_filename <- 'Technical Test Data and Information/PUMS_Data_Dictionary_2019.csv'

dictionaries <- readr::read_lines(dict_filename) %>% 
  unique() %>% 
  split(str_detect(., '^NAME')) %>% # identify each dataset
  purrr::set_names(c('values', 'names')) %>% 
  purrr::map2(.y = list(val_columns = c('field_type',
                                        'column_name',
                                        'column_type',
                                        'field_length',
                                        'starting_value',
                                        'ending_value',
                                        'description'),
                        name_columns = c('field_type',
                                         'column_name',
                                         'column_type',
                                         'field_length',
                                         'description')),
              .f = ~data.table::fread(text = c(paste(.y, collapse = ','),
                                               .x)))
```

Additionally, a mark for the dataset was added. The first variable of the Person dataset is SPORDER, so any entry before it belongs to the Housing Unit data.

```{r dict_mark_h_or_p}
# find the index of SPORDER in both the $names and $values tables
sporder_index <- dictionaries %>% 
  purrr::map(~which(.x$column_name %chin% 'SPORDER'))

# map the corresponding dataset flag to each row
walk2(.x = dictionaries, 
      .y = sporder_index,
      .f = ~.x[, record_number := 1:.N
             ][, dataset := data.table::fifelse(test = record_number < .y,
                                           yes = 'H',
                                           no = 'P')]) 
```

With this code, this:

```{r, echo=FALSE}
readr::read_lines(dict_filename)[1:20] %>% paste('\n') %>%  cat()
```

Turns into these:

**Column dictionary**

```{r, echo=FALSE}
knitr::kable(head(dictionaries$names))
```

**Value dictionary**

```{r, echo=FALSE}
knitr::kable(head(dictionaries$values))
```

And for future convenience, this process can be packaged into a single call:

```{r}
dict_filename <- 'Technical Test Data and Information/PUMS_Data_Dictionary_2019.csv'
dicts <- parse_dictionaries(dict_filename)

str(object = dicts, give.atr = F, level = 2, vec.len = 2)

```

## Data Loading and joining

We load both datasets into memory. We need to take no memory considerations because the data is rather small. We remove the 'RT' column because it is on both files but with different values for the same serial numbers, so it would obtruse the join process.

```{r load_data}
# load person data
per <- data.table::fread('Technical Test Data and Information/psam_p48.csv') %>% 
  discard_cols(cols = 'RT')
# 272,776 rows

# load housing unit data
hu <- data.table::fread('Technical Test Data and Information/psam_h48.csv') %>% 
  discard_cols(cols = 'RT')
# 123,710 rows

```

Since the amount of columns of each table is quite extensive, we will first join both tables, and seek to keep only the variables that both are potentially relevant to the study at hand, and have a reasonable amount of valid data. Although the documentation suggests joining the tables by 'SERIALNO', the records also share several location variables, which will be also used on the join criteria to avoid duplication.

```{r merge_data}
join_cols <- intersect(colnames(per), colnames(hu))
# this is c("SERIALNO", "DIVISION", "PUMA", "REGION", "ST", "ADJINC")

# set data.table keys for fast joining
data.table::setkeyv(per, join_cols)
data.table::setkeyv(hu,  join_cols)

# data.table left join
dat <- data.table::merge.data.table(x = per, hu, all.x = TRUE)

# remove the separate tables from memory
rm(per)
rm(hu)

# map all character variables as.character:

# select columns that are specified as character columns in the dictionary 
character_cols <- dictionaries$names[column_type == 'C']$column_name %>% intersect(colnames(dat))

# coerce to character by reference
dat[, (character_cols) := purrr::map(.SD, as.character), .SDcols = character_cols]
```

## Column discarding

The joint table comprehends 517 variables, but we can quickly reduce the size of the dataset with three simple ideas:

-   Over 100 of those columns are weight values for computing representative aggregates, which is not part of the scope of this study.

```{r remove_wieght_columns}
dat <- discard_cols(dt = dat, cols = c('^WGTP', '^PWGTP'), regex = TRUE)
```

-   We will not discriminate on whether the item comes from a true response or an [allocated response](https://www.census.gov/programs-surveys/acs/methodology/sample-size-and-data-quality/item-allocation-rates-definitions.html).

```{r remove_flag_allocation}
flag_columns <- dictionaries$names[str_detect(description, 'allocation flag')]$column_name
dat <- discard_cols(dt = dat, cols = flag_columns, regex = TRUE)
```

-   We could remove columns with a single constant value.

```{r remove_constant_columns}
unique_values <- data.table(column = colnames(dat),
                            unique_values = purrr::map_int(dat, uniqueN)
                            )[unique_values == 1
                            ][, description := describe_column(column)][]

knitr::kable(unique_values)
```

As expected, these columns mostly correspond to geographical entries that are all the same for all Texan residents, and adjustment coefficients that are all the same for every observation done on the same time period.

```{r remove_constant_columns2}
dat <- discard_cols(dt = dat, cols = unique_values$column)
```

With that, we went from 517 to 218 variables, a much more manageable amount of columns to consider. We can now proceed to decode all categorical variables. The strategy here is to fetch the values specified in the data dictionary, and rewrite the values of each column by reference for the minimum memory overhead possible.

## Valid data coverage

Let's proceed examining the percentage of valid data of all the other variables.

```{r}
na_perc <- na_percentage(dat) %>% 
  data.table::merge.data.table(y = dictionaries$names[, .(column_name, column_type, description)],
                               by.x = 'column',
                               by.y = 'column_name')

na_perc[order(-na_perc)] %>% head(10) %>% knitr::kable()

```

Most variables with high NA coverage are ones where there exists a field for "No data/Does not apply" but it is not explicitly stated, instead it is read as an NA (which is a reasonable space saving decision.) But that means that there is no need to discard most of these variables they are easily mapped into the default response 'b' == 'Does not apply 'for categorical variables, and 0 would be appropriate for most numerical variables since those represent costs, payments and counts.

```{r data_imputation}
numeric_impute <- dictionaries$names[column_type == 'N']$column_name %>%
  intersect(colnames(dat)) %>% 
  setdiff('PINCP') # our target variable deserves special consideration 

na_fill(dt = dat,  
        cols = numeric_impute,
        replacement = 0)

# for categoricals, only the variables that have an explicit 'b' = 'Does not apply' field will be replaced.
categorical_impute <- dictionaries$names[column_type == 'C']$column_name %>%
  intersect(colnames(dat)) %>% 
  intersect(dictionaries$values[column_type == 'C' & starting_value == 'b']$column_name)

na_fill(dt = dat,  
        cols = categorical_impute,
        replacement = 'b')

```

Recomputing the valid data coverage metrics, we see much better metric for most variables, only keeping the categorical variables that are not mappable to a 'b' field, and our target variable that we decided not to impute.

```{r}
na_perc <- na_percentage(dat) %>% 
  data.table::merge.data.table(y = dictionaries$names[, .(column_name, column_type, description)],
                               by.x = 'column',
                               by.y = 'column_name')

na_perc[na_perc > 0] %>% head(10) %>% knitr::kable()

```

## Categorical variable decoding

Now, with all most variables rescued, we can proceed and map them into their true values. We first create the list of categorical variables to be decoded:

```{r pre_decoding}
# select the categorical variables according to the dictionary
columns_to_decode <- dictionaries$names[column_type %chin% 'C']$column_name 

# do not decode range categorical variables (ie where starting_value != ending_value)
# These are c("SERIALNO", "SERIALNO", "PUMA", "MIGPUMA", "POWPUMA", "RACNUM")
exclude_from_decoding <- 
  dictionaries$values[column_type %chin% 'C' & starting_value != ending_value]$column_name
columns_to_decode %<>% 
  setdiff(exclude_from_decoding) 

# and remove the columns we already removed from the dataset
columns_to_decode %<>% 
  intersect(colnames(dat))
```

The following function then rewrites the content of all coded columns into their explicit values (eg, for the 'AGS' column (Sales of Agriculture Products): the value '1' will be changed to 'None', '2' will be changed to '\$1 - \$999', and so on.)

```{r, variable_decoding}
# and define a function that decodes a column
decode_categoricals <- function(col, dat){
  # select the dictionary entries speciying column values
  column_mapping <- dictionaries$values[column_name %chin% col & starting_value == ending_value,
                                        .(starting_value, description)] %>% 
    data.table::setkey(starting_value)
  
  # data.table 'left join' by reference
  # technically this is a mutate by reference, not a join
  matching_statement <- paste0(col, '==', 'starting_value')
  setkeyv(dat, col)
  dat[column_mapping, on = matching_statement, (col) := i.description]
}

tictoc::tic() 
# apply the function to all categorical columns
columns_to_decode %>% walk(decode_categoricals, dat = dat)
tictoc::toc()
```

On this execution time, the function was able to compute the mappings of all 169 coded columns, rewriting over the same pointers which also means no memory overhead.

With this final step, not only do we have a workable data file that needs no external reference (besides column names, whose descriptions are too long to use as column names directly), but this also can be wrapped into into a single function, allowing us to efficiently parse all past and future PUMS files that follow the same structure.

```{r full_preprocess_function, warning=FALSE, message=FALSE}
per_fn <- 'Technical Test Data and Information/psam_p48.csv'
hu_fc <- 'Technical Test Data and Information/psam_h48.csv'
dict_filename <- 'Technical Test Data and Information/PUMS_Data_Dictionary_2019.csv'

datt <- pums_for_individual_modelling(person_filename = per_fn,
                                     housing_unit_filename = hu_fc,
                                     dictionary_filename = dict_filename)
```

# Exploratory data analysis

## Skim through the data

We proceed to examine our processed table. We can quickly skim through all numerical variables with the skim() function. Categorical variables were omitted to avoid a very large table output.

```{r skim}
# create a copy of dat with a snippet of the variable description pasted into each column name
dat_with_descriptions <- data.table::copy(dat)
desc_index <- match(colnames(dat), dictionaries$names$column_name)
desc_colnames <- paste0(colnames(dat), ': ', str_sub(dictionaries$names$description[desc_index],1,30))
colnames(dat_with_descriptions) <- desc_colnames

skimr::skim(keep(dat_with_descriptions, is.numeric))
```

From this, we can see that most numerical variables are right skewed, which is consistent with the imputation process assigning them 0 on all their missing data fields. This, however, is also consistent with the interpretation of the variables, so it is not considered data distortion as much as it is data completion.

## Target variable analysis

As we have an explicit target variable, we can center our data exploration in function of the relationship between each variable and our y variable: PINCP. This is a continuous numerical variable, which means we are dealing with a regression problem. We start off by understanding our variable:

```{r income_num_summ}
num_summary(dt = dat, col = 'PINCP') %>% knitr::kable()
```

From these statistics, we can see that the variable reaches negative values, which is a valid data entry if the respondent had negative income. There is no documentation on how the PINCP variable is reported or computed (which would also defeat the purpose of this study.), so we have no solid reason to discard those records.

On the other hand, and as previously pointed out in prior sections, 18% of the PINCP fields are empty, and there are records with 0 assigned to that column, hence it would be reckless to conclude that missing data means zero. We will remove those observations from the data.

```{r remove_income_nas}
dat <- dat[!is.na(PINCP)]
```

We reach then 222,893 valid observations of an individual.

And now we can proceed with visualizing our variable. As expected with income variables, its distribution is heavily asymmetrical, heavily right skewed even after the variable being bounded at 1,166,600.

```{r PINCP_density}
chronicle::make_density(dt = dat, value = 'PINCP', static = FALSE)
```

Then, we can study the relationship between PINCP and all other numerical variables through a linear correlation table

```{r correlations}
# select numeric columns only
nums <- dat %>% purrr::keep(is.numeric) %>% colnames()

# compute correlations against the individual ncome variable, and count NA values 
income_cor <- data.table(column_name = nums,
                  income_cor = purrr::map_dbl(.x = nums,
                                              .f = ~cor(na.omit(dat[, .(PINCP, get(.x))]))[1,2] %>% round(2)),
                  na_perc = purrr::map_dbl(.x = nums, 
                                           .f = ~mean(is.na(dat[[.x]])))
                  )[, description := describe_column(column_name)]

income_cor[order(-abs(income_cor))] %>% knitr::kable()
```

## Numerical variables

Because most numerical variables are likely to be higher on better-off individuals, it stands to reason that most of them are positively correlated with our income target variable. We will however remove 'PERNP', 'WAGP', since those two variables are almost explicitly our target variable and defeats the purpose of the exercise.

```{r discard_wage_earnings_variables}
dat <- discard_cols(dt = dat, cols = c('PERNP', 'WAGP'))
```

We also want to see the correlation between all other variables. Due to the large amount of variables still present, we opt for an interactive heatmap to glance through the data.

```{r num_correlations, fig.width=10, fig.height=8}
cor_dat <- cor(keep(dat, is.numeric)) %>% 
  round(2) %>% 
  as.data.table(keep.rownames = TRUE) %>% 
  data.table::melt.data.table(id.vars = 'rn', measure.vars = colnames(.)[-1]) %>% 
  setnames(old = c('rn', 'variable'), new = c('x', 'y'))

cor_dat[, x := factor(x, levels = sort(unique(x)))
      ][, y := factor(y, levels = sort(unique(x), decreasing = TRUE))]


(ggplot(data = cor_dat, 
                 mapping = aes(x = x, y = y, fill = value)) +
    geom_tile() +
    theme_minimal() +
    theme(panel.background = element_rect(fill = "transparent", colour = NA),
                   plot.background =  element_rect(fill = "transparent", colour = NA), 
                   axis.title = element_blank(),
                   axis.text.x = element_blank()) +
    viridis::scale_fill_viridis(option = 'magma')) %>% 
  plotly::ggplotly()

```

## Categorical Variables

For categoricals, a good approach would be visualizing the densities of the target variable for each class, but since we have around 300 categorical variables, it is not feasible to do it on every single one in the given time frame. We manually select a few variables that seem sensible to single out. The variables selected are:

```{r categorical_tables, echo=FALSE}
categoricals_to_explore <- c("HINS4",
                             "MAR",
                             "WKL",
                             "ESR",
                             "SCIENGP",
                             "SCIENGRLP",
                             "FS",
                             "HISPEED",
                             "TEN",
                             "KIT",
                             "SMX") %>% 
  sort() %>% 
  purrr::set_names(.)

data.table(Variable = categoricals_to_explore 
         )[order(Variable)
         ][, Description := describe_column(Variable)] %>%
  kable()
```

We then proceed to make box plots of all the selected variables. Because PINCP has around 10,000 records (5% of all observations) above 150,000, we will truncate the charts at that value to still be able to easily grasp differences in the medians of the categorical groups, without the plot being overtaken by the higher and unusual values.

```{r, warning=FALSE, message=FALSE}
categorical_plots <- categoricals_to_explore %>% 
  purrr::map2(.x = categoricals_to_explore, 
              .y = describe_column(categoricals_to_explore), 
              .f = ~chronicle::make_boxplot(dt = dat[PINCP < 150000, .(PINCP, (.x) = get(.x))],
                                            value = 'PINCP', 
                                            groups = .x,
                                            jitter = FALSE, 
                                            x_axis_label = .y, 
                                            static = FALSE)) %>%
  set_names(categoricals_to_explore)
```

### Employment status recode

```{r, warning=FALSE, echo=FALSE}
categorical_plots$ESR
```

### Yearly food stamp/Supplemental Nutrition Assistance Program (SNAP) recipiency

```{r, warning=FALSE, echo=FALSE}
categorical_plots$FS
```

### Medicaid, Medical Assistance, or any kind of government-assistance plan for those with low incomes or a disability

```{r, warning=FALSE, echo=FALSE}
categorical_plots$HINS4
```

### Broadband (high speed) Internet service such as cable, fiber optic, or DSL service

```{r, warning=FALSE, echo=FALSE}
categorical_plots$HISPEED
```

### Complete kitchen facilities

```{r, warning=FALSE, echo=FALSE}
categorical_plots$KIT
```

### Marital status

```{r, warning=FALSE, echo=FALSE}
categorical_plots$MAR
```

### Field of Degree Science and Engineering Flag - NSF Definition

```{r, warning=FALSE, echo=FALSE}
categorical_plots$SCIENGP
```

### Field of Degree Science and Engineering Related Flag - NSF Definition

```{r, warning=FALSE, echo=FALSE}
categorical_plots$SCIENGRLP
```

### Second or junior mortgage or home equity loan status

```{r, warning=FALSE, echo=FALSE}
categorical_plots$SMX
```

### Tenure

```{r, warning=FALSE, echo=FALSE}
categorical_plots$TEN
```

### When last worked

```{r, warning=FALSE, echo=FALSE}
categorical_plots$WKL
```

As expected, most of these variables give a clear quantile/median distinction in regards to the individual income. These variables were chosen because they are proxies for poverty/quality of life, and have few distinct values to comfortably be visualized.

# Feature Selection

A good feature selection process should include both subject-matter expertise and sensible statistical behaviors. It also should take into account the use on the model and whether or not it will help perpetuate discrimination over protected groups like ethnicity, gender, nationality, migratory status, religious belief, disability, and so on. Since the purpose of the model was not disclosed as part of the briefing, this type of variables will not be taken into account on the study, and will be explicitly removed from the modeling dataset:

```{r, echo=FALSE}
protected_categories <- c("CIT",
                          "CITWP",
                          "DREM",
                          "HINS7",
                          "SEX",
                          "DIS",
                          "HISP",
                          "NATIVITY",
                          "RAC1P",
                          "RAC2P",
                          "RAC3P",
                          "RACAIAN",
                          "RACASN",
                          "RACBLK",
                          "RACNH",
                          "RACNUM",
                          "RACPI",
                          "RACSOR",
                          "RACWHT",
                          "CPLT",
                          "FES")
data.table(protected_categories, description = describe_column(protected_categories)) %>% knitr::kable()
```

We can also build hand-picked variables

```{r}
dat[, hu_per_capita_income := HINCP/NP]
```

```{r, echo=FALSE}
dat <-  <- (dt = dat,
                    cols =  protected_categories)
```

Since we are confident about our data cleansing process, and good data health from ACS practices, we can rely on H2O's randomForest heuristics to drop 'bad columns'. The main reason why a variable might be dropped is it being a categorical variable with a large quantity of distinct values, since ensemble tree models tend to memorize the categories and end up overfitting the data even in a small number of epochs.

We first setup a local H2O cluster, allocate it 10GB of RAM and split the data into a train and testing set. The training will be done with cross validation, but we stil want to test its performance on unseen data.

```{r h2o_setup, warning=FALSE, message=FALSE}
library(h2o)

#set up the H2O local cluster
h2o.init(max_mem_size = '16G') # allocating 16GB of RAM

# feed all variables to the model
x <- setdiff(colnames(dat), c('PINCP', 'SERIALNO', 'SPORDER'))
y <- 'PINCP'

# import dataset into h2o cluster
model_dat <- as.h2o(dat)

dat_split <- h2o.splitFrame(data = model_dat, ratios = 0.8, seed = 1)
train <- dat_split[[1]]
test <- dat_split[[2]]

```

Then we start off with a random forest model with H2O's very reasonable preset values (ntrees = 50, max_depth = 20, nbins = 20, no early stopping)

# Model calibration

```{r rf, warning=FALSE, message=FALSE}
# build a cross validated model
rf <- h2o.randomForest(x = x,
                       y = y,
                       training_frame = train,
                       nfolds = 10, 
                       max_runtime_secs = 100, 
                       seed = 1)
```

```{r}
rf@model$model_summary %>% knitr::kable()
```

And we can see there is stability in the cross validation metrics

```{r}
rf@model$cross_validation_metrics_summary
```

And we can see that the performance on the unseen data is basically identical to the cross validation metrics

```{r model_performance}
# check performance over test set
rf@model$cross_validation_metrics
h2o.performance(model = rf, newdata = test)
```

While MAE and RMSE give an idea of the range of values that might be predicted, they are not ideal to understand the fit for different income bands, since being 11,000 USD off means quite different things for people in the first vs last quintiles. Hence we propose using the median relative error to see the overall performance across different income bands.

```{r prediction}
# predict over the test set
pred <- h2o.predict(object = rf, 
                   newdata = test)

# compute relative error for each observation
pred_frame <- cbind(as.data.table(test)[, .(PINCP)],
                    as.data.table(pred)
                    )[, absolute_error := abs(predict-PINCP)
                    ][, relative_error := absolute_error/PINCP]

# check the performance in each quintile
discretize_column(dt = pred_frame,
                  amount_column = 'PINCP',
                  band_cuts = quantile(pred_frame$PINCP, probs = seq(0, 1, .2)), 
                  bands_column_name = 'income_band')

pred_frame[, .(population_size = .N,
               median_absolute_error = median(absolute_error),
               median_relative_error = median(relative_error)),
           by = income_band][order(income_band)] %>% knitr::kable()
```

And to see the performance without evening out good and bad results, we can draw a line in the sand by defining a 'good' result as a predicted value with a relative error below .15, and see the percentage of good results in each income band.

```{r}
# flag good predictions
pred_frame[, good_prediction := relative_error < .20]

pred_frame[, .(perc_good_pred = mean(good_prediction)), by = income_band]
```

In conclusion, our model is markedly competent and stable across the different positive income bands.

Now, we proceed to revise which variables the model took into account. We do this by reviewing the model's [variable importance](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html#:~:text=0%20and%201.-,Variable%20Importance%20Calculation%20(GBM%20%26%20DRF),(decreased)%20as%20a%20result.) ranking.

```{r}
var_importance <- as.data.table(h2o.varimp(rf))[, cumulative_perc := cumsum(percentage)
                                              ][, relative_importance := NULL
                                              ][, description := describe_column(variable)]

head(var_importance, 10) %>% knitr::kable()
```

These all are reasonably related to personal income: some of them explicitly state incomes (retirement, self-employment) while others relate to the stage of the traditional life cycle of the individual (age, vehicle occupancy).

Overall, we have obtained a

# Appendix

## The {chronicle} package

This package was developed by Philippe Heymans, and aims to provide a seamless experience while building sophisticated and interactive visualizations in R, handling most of their challenges though its collection of helper functions. For further detail, please visit its [github page](https://github.com/pheymanss/chronicle).

## Exercise-structured answers

As consideration to the audience, this report is presented as a product delivery instead of homework. This section however, provides a Q&A structure based on the Technical Test Specs provided.

#### 1. If the data is too large for the analysis, what will you do to reduce the size of the data, or work with the full dataset? Explain the steps you used.

R/. At 253MB (70MB + 183MB), this dataset should not present issues on any reasonably modern hardware setup, and that is precisely the reason why the PUMS dataset exists. Assuming reasonable stability in the size of the fields, the size of the full dataset would range around 25GB, which would be quite more demanding, but still not enough to be a limiting factor of the steps carried out in this particular study. (Mostly automatic) feature selection methods were used, but those were motivated by good statistical practices, not data size problems. A uausl quick win is to identify columns that are immediately irrelevant to the problem at hand (eg, sample weights and allocation flags), which let us go from 500 to 200 variables without sacrificing a thing.

If this was a national study done with the full ACS dataset instead of a Texas-focused study with a 1% sample, then the size of the raw dataset would indeed be a limiting factor, and loading the entire dataset into memory would not be feasible on consumer-grade hardware. Supposing that it is not worth it to load the dataset into a distributed filesystem setup, the healthier approach would be to start discarding portions of the data that would not be used in the model. If discarding all those columns was not enough for the full dataset, then performing a sample of the data could suffice to diagnose, re-structure and calibrate the desired model. However, assuring a representative sampling can potentially be more time consuming than the study itself.

#### 2. How will you double-check the individual's income in a household against the overall household income?

```{r check_p_h_income}
dat[!is.na(HINCP), .(PINCP, HINCP)]
dat[PINCP > HINCP, .(PINCP, HINCP)]
```

#### 3. What data makes sense to use as potential features for a model, and which data does not?

#### 4. What data cleansing techniques did you use and why?

#### 5. Prepare your data for model development, explaining the steps you used for that preparation.

#### 6. Make sure to document all of the steps you used for building the model(s).

#### 7. Include any relevant metrics that you use to determine if the model is acceptable or not.

#### 8. Be ready to defend your results. It is fine if you cannot build a model that will successfully predict income. You just need to be able to explain why that is the case.

### Extra marks

#### 1. Identify who is the head of the household/main wage earner. Explain what criteria you used to do so.

```{r}
# dat[, highest_earner := max(PINCP), by = SERIALNO][, main_earger := PINCP == highest_earner]
```

## Helper functions

This is a small collection of facilitator functions meant to make the output more readable, and avoid distractions from formatting or complex code.

```{r}

# fetch the column description from the NAME entries of the data dictionary
describe_column <- function(col = NULL){
  descrip_index <- match(col, dictionaries$names$column_name)
  descr <- dictionaries$names$description[descrip_index]
  descr[is.na(descr)] <- 'Not on ACS dictionary'
}

# quickly format percentage without relying on the {scales} package
fp <- function(x){
  round(x*100,2) %>% paste0('%')
}

# run summary statistics over a numerical column of a data.table (optionally grouped)
num_summary <- function(dt, col, by = NULL){
  # rename the column to a generic variable (to avoid eval() or get() calls)
  data.table::setnames(x = dt, old = col, new = 'value')
  
  # perform the computations
  summ <- dt[, .(count = sum(!is.na(value)),
               distinct_values = uniqueN(value),
               na_count = sum(is.na(value)),
               na_perc = sum(is.na(value))/length(value),
               sum = sum(value, na.rm = TRUE),
               mean = mean(value, na.rm = TRUE),
               sd = sd(value, na.rm = TRUE),
               min = min(value, na.rm = TRUE),
               percentile.1 = quantile(value, 0.1, na.rm = TRUE),
               percentile.2 = quantile(value, 0.2, na.rm = TRUE),
               percentile.3 = quantile(value, 0.3, na.rm = TRUE),
               percentile.4 = quantile(value, 0.4, na.rm = TRUE),
               percentile.5 = quantile(value, 0.5, na.rm = TRUE),
               percentile.6 = quantile(value, 0.6, na.rm = TRUE),
               percentile.7 = quantile(value, 0.7, na.rm = TRUE),
               percentile.8 = quantile(value, 0.8, na.rm = TRUE),
               percentile.9 = quantile(value, 0.9, na.rm = TRUE),
               percentile.95 = quantile(value, 0.95, na.rm = TRUE),
               percentile.975 = quantile(value, 0.975, na.rm = TRUE),
               percentile.99 = quantile(value, 0.99, na.rm = TRUE),
               max = max(value, na.rm = TRUE)),
             by = by]
  
  # rename the column back to its original name
  data.table::setnames(x = dt, old = 'value', new = col)
  
  return(summ)
}

# compute pearson correlation while skipping NAs in the y vector
cor_na.rm <- function(x,y){
  na_index <- is.na(y)
  cor(x[!na_index], y[!na_index])
}


# compute the count and percentage of missing entries for each column of a data.frame 
na_percentage <- function(dt){
  data.table(column = colnames(dt),
             na_count = purrr::map_int(.x = dt, 
                                       .f = ~sum(is.na(.x))
                                       ))[, na_perc := na_count/nrow(dt)][]
}

# removes all columns matching a character vector (optionally regular expressions)
discard_cols <- function(dt, cols, regex = FALSE){
  if(!regex){
    dt %>% purrr::discard(colnames(.) %in% cols)
  }else{
    dt %>% purrr::discard(str_detect(colnames(.), paste(cols, collapse = '|')))
  }
}

# removes all columns but the ones matching a character vector (optionally regular expressions)
keep_cols <- function(dt, cols, regex = FALSE){
  if(regex){
    cols <- stringr::str_subset(colnames(dt), paste(cols, collapse = '|'))
  }else{
    # warn if any column passed is not present on dt
    if(!all(cols %in% colnames(dt))){
      warning(paste0('\nNot all columns found on ', deparse(substitute(dt)), '. Missing columns are:',
                     paste(cols[!(cols %in% colnames(dt))], collapse = ', ')))
    }
  }
  dt %>% purrr::keep(colnames(.) %in% cols) %>% data.table::setcolorder(cols)
}


na_fill = function(dt, cols, replacement = 0) {
  # match the indexes of the columns specified by the user
  index <- match(cols, colnames(dt))
  purrr::walk(index, ~data.table::set(dt, which(is.na(dt[[.x]])), .x, replacement))
}


# parse the two separate specifications entangled in the ACS dictionary file
parse_dictionaries <- function(dict_filename){
  dictionaries <- readr::read_lines(dict_filename) %>%
    unique() %>%
    split(stringr::str_detect(., '^NAME')) %>% # identify each dataset
    purrr::set_names(c('values', 'names')) %>%
    purrr::map2(.y = list(val_columns = c('field_type',
                                          'column_name',
                                          'column_type',
                                          'field_length',
                                          'starting_value',
                                          'ending_value',
                                          'description'),
                          name_columns = c('field_type',
                                           'column_name',
                                           'column_type',
                                           'field_length',
                                           'description')),
                .f = ~data.table::fread(text = c(paste(.y, collapse = ','),
                                                 .x)))
  
  # find the index of SPORDER in both the $names and $values tables
  sporder_index <- dictionaries %>%
    purrr::map(~which(.x$column_name %chin% 'SPORDER'))
  
  # map the corresponding dataset flag to each row
  walk2(.x = dictionaries,
        .y = sporder_index,
        .f = ~.x[, record_number := 1:.N
        ][, dataset := data.table::fifelse(test = record_number < .y,
                                           yes = 'H',
                                           no = 'P')])
  return(dictionaries)
}

pums_for_individual_modelling <- function(person_filename,
                                         housing_unit_filename,
                                         dictionary_filename){

  tictoc::tic('PUMS files successfully parsed')
  # 0. load and join data ---
    
  # load person data
  per <- data.table::fread('Technical Test Data and Information/psam_p48.csv') %>% 
    discard_cols(cols = 'RT')

  # load housing unit data
  hu <- data.table::fread('Technical Test Data and Information/psam_h48.csv') %>% 
    discard_cols(cols = 'RT')

  dictionaries <- parse_dictionaries(dict_filename)
  
  join_cols <- intersect(colnames(per), colnames(hu))
  # this is c("SERIALNO", "DIVISION", "PUMA", "REGION", "ST", "ADJINC")
  
  # set data.table keys for fast joining
  data.table::setkeyv(per, join_cols)
  data.table::setkeyv(hu,  join_cols)
  
  # data.table left join
  dat <- data.table::merge.data.table(x = per, hu, all.x = TRUE)
  
  # remove the separate tables from memory
  rm(per)
  rm(hu)
  
  # 1. discard columns ----
  
  # weight columns
  dat <- discard_cols(dt = dat, cols = c('^WGTP', '^PWGTP'), regex = TRUE)
  
  # flag columns
  flag_columns <- dictionaries$names[str_detect(description, 'allocation flag')]
  dat <- discard_cols(dt = dat, cols = flag_columns, regex = TRUE)
  
  # constant_columns
  unique_values <- data.table(column = colnames(dat),
                              unique_values = purrr::map_int(dat, uniqueN)
                              )[unique_values == 1]
  
  dat <- discard_cols(dt = dat, cols = unique_values$column)
  
  # 2. data imputation ----
  numeric_impute <- dictionaries$names[column_type == 'N']$column_name %>%
    intersect(colnames(dat)) %>% 
    setdiff('PINCP') # our target variable deserves special consideration 
  
  na_fill(dt = dat,  
          cols = numeric_impute,
          replacement = 0)
  
  # for categoricals, only the variables that have an explicit 'b' = 'Does not apply' field will be replaced.
  categorical_impute <- dictionaries$names[column_type == 'C']$column_name %>%
    intersect(colnames(dat)) %>% 
    intersect(dictionaries$values[column_type == 'C' & starting_value == 'b']$column_name)
  
  na_fill(dt = dat,  
          cols = categorical_impute,
          replacement = 'b')
  
  
  # 3. categorical variable decoding ----
  
  # select the categorical variables according to the dictionary
  columns_to_decode <- dictionaries$names[column_type %chin% 'C']$column_name 
  
  # do not decode range categorical variables (ie where starting_value != ending_value)
  # These are c("SERIALNO", "SERIALNO", "PUMA", "MIGPUMA", "POWPUMA", "RACNUM")
  exclude_from_decoding <- 
    dictionaries$values[column_type %chin% 'C' & starting_value != ending_value]$column_name
  columns_to_decode %<>% 
    setdiff(exclude_from_decoding) 
  
  # and remove the columns we already removed from the dataset
  columns_to_decode %<>% 
    intersect(colnames(dat))
  
  # set all categorical variables as character vectors to avoid type inconsistencies 
  dat[, (columns_to_decode) := purrr::map(.SD, as.character), .SDcols = columns_to_decode]
  
  columns_to_decode %>% walk(decode_categoricals, dat = dat)
  
  tictoc::toc()
  
  return(dat)
}

# decode categorical columns according to the values dictionary
decode_categoricals <- function(col, dat){
  # select the dictionary entries speciying column values
  column_mapping <- dictionaries$values[column_name %chin% col & starting_value == ending_value,
                                        .(starting_value, description)] %>% 
    data.table::setkey(starting_value)
  
  # data.table 'left join' by reference
  # technically this is a mutate by reference, not a join
  matching_statement <- paste0(col, '==', 'starting_value')
  setkeyv(dat, col)
  dat[column_mapping, on = matching_statement, (col) := i.description]
}


# create a new column marking the value band of each observation wrt a continuous column 
discretize_column <- function(dt,
                              amount_column,
                              band_cuts,
                              bands_column_name) {
  
  amount_column %<>% as.character()
  
  # create empty column
  data.table::setDT(dt)
  dt[, (bands_column_name) := NA_character_]
  
  # build index for the bands
  run <- seq_along(band_cuts)
  band_length <- length(band_cuts)
  
  # create names for the bands
  band_names <- paste0(band_cuts[-band_length], '-', band_cuts[-1])
  
  # assign band to each value
  purrr::walk(run, ~data.table::set(dt,
                                    which(band_cuts[.x] <= dt[[amount_column]] &
                                            dt[[amount_column]] <= band_cuts[.x + 1]),
                                    bands_column_name,
                                    band_names[.x])[])
  
  dt[, (bands_column_name) := factor(x = get(bands_column_name), levels = band_names)]
}


```

## Session Info

```{r}
sessionInfo()
```
