---
title:  Individual Income Model
date: "`r Sys.Date()`"
author:  Philippe Heymans Smith
output:
  rmdformats::downcute:
    code_folding: show
    fig_width: 8
    fig_height: 4
    use_bookdown: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
static_plots = FALSE
options(scipen = 999)
set.seed(1)
```

```{r helper_functions_inv, include=FALSE}

# fetch the column description from the NAME entries of the data dictionary
describe_column <- function(col = NULL){
  descrip_index <- match(col, dictionaries$names$column_name)
  descr <- dictionaries$names$description[descrip_index]
  
  if(any(is.na(descr))){
    descr[is.na(descr)] <- 'Not on ACS dictionary'
  }
  return(descr)
}

# run summary statistics over a numerical column of a data.table (optionally grouped)
num_summary <- function(dt, col, by = NULL){
  # rename the column to a generic variable (to avoid eval() or get() calls)
  data.table::setnames(x = dt, old = col, new = 'value')
  
  # perform the computations
  summ <- dt[, .(count = sum(!is.na(value)),
               distinct_values = uniqueN(value),
               na_count = sum(is.na(value)),
               na_perc = sum(is.na(value))/length(value),
               sum = sum(value, na.rm = TRUE),
               mean = mean(value, na.rm = TRUE),
               sd = sd(value, na.rm = TRUE),
               min = min(value, na.rm = TRUE),
               percentile.25 = quantile(value, 0.25, na.rm = TRUE),
               percentile.50 = quantile(value, 0.5, na.rm = TRUE),
               percentile.75 = quantile(value, 0.75, na.rm = TRUE),
               percentile.975 = quantile(value, 0.975, na.rm = TRUE),
               percentile.99 = quantile(value, 0.99, na.rm = TRUE),
               max = max(value, na.rm = TRUE)),
             by = by]
  
  # rename the column back to its original name
  data.table::setnames(x = dt, old = 'value', new = col)
  
  return(summ)
}

# compute the count and percentage of missing entries for each column of a data.frame 
na_percentage <- function(dt){
  data.table(column = colnames(dt),
             na_count = purrr::map_int(.x = dt, 
                                       .f = ~sum(is.na(.x))
                                       ))[, na_perc := na_count/nrow(dt)][]
}

# removes all columns matching a character vector (optionally regular expressions)
discard_cols <- function(dt, cols, regex = FALSE){
  if(!regex){
    dt %>% purrr::discard(colnames(.) %in% cols)
  }else{
    dt %>% purrr::discard(str_detect(colnames(.), paste(cols, collapse = '|')))
  }
}


na_fill = function(dt, cols, replacement = 0) {
  # match the indexes of the columns specified by the user
  index <- match(cols, colnames(dt))
  purrr::walk(index, ~data.table::set(dt, which(is.na(dt[[.x]])), .x, replacement))
}


# parse the two separate specifications entangled in the ACS dictionary file
parse_dictionaries <- function(dict_filename){
  dictionaries <- readr::read_lines(dict_filename) %>%
    unique() %>%
    split(stringr::str_detect(., '^NAME')) %>% # identify each dataset
    purrr::set_names(c('values', 'names')) %>%
    purrr::map2(.y = list(val_columns = c('field_type',
                                          'column_name',
                                          'column_type',
                                          'field_length',
                                          'starting_value',
                                          'ending_value',
                                          'description'),
                          name_columns = c('field_type',
                                           'column_name',
                                           'column_type',
                                           'field_length',
                                           'description')),
                .f = ~data.table::fread(text = c(paste(.y, collapse = ','),
                                                 .x)))
  
  # find the index of SPORDER in both the $names and $values tables
  sporder_index <- dictionaries %>%
    purrr::map(~which(.x$column_name %chin% 'SPORDER'))
  
  # map the corresponding dataset flag to each row
  walk2(.x = dictionaries,
        .y = sporder_index,
        .f = ~.x[, record_number := 1:.N
        ][, dataset := data.table::fifelse(test = record_number < .y,
                                           yes = 'H',
                                           no = 'P')])
  return(dictionaries)
}

# decode categorical columns with the values provided in the VAL dictionary.
decode_categoricals <- function(col, dat){
  # select the dictionary entries speciying column values
  column_mapping <- dictionaries$values[column_name %chin% col & starting_value == ending_value,
                                        .(starting_value, description)] %>% 
    data.table::setkey(starting_value)
  
  # data.table 'left join' by reference
  # technically this is a mutate by reference, not a join
  matching_statement <- paste0(col, '==', 'starting_value')
  setkeyv(dat, col)
  dat[column_mapping, on = matching_statement, (col) := i.description]
}

pums_for_individual_modelling <- function(person_filename,
                                          housing_unit_filename,
                                          dictionary_filename){

  tictoc::tic('PUMS files successfully parsed')
  # 0. load and join data ---
    
  # load person data
  per <- data.table::fread('Technical Test Data and Information/psam_p48.csv') %>% 
    discard_cols(cols = 'RT')

  # load housing unit data
  hu <- data.table::fread('Technical Test Data and Information/psam_h48.csv') %>% 
    discard_cols(cols = 'RT')

  dictionaries <- parse_dictionaries(dict_filename)
  
  join_cols <- intersect(colnames(per), colnames(hu))
  # this is c("SERIALNO", "DIVISION", "PUMA", "REGION", "ST", "ADJINC")
  
  # set data.table keys for fast joining
  data.table::setkeyv(per, join_cols)
  data.table::setkeyv(hu,  join_cols)
  
  # data.table left join
  pums <- data.table::merge.data.table(x = per, hu, all.x = TRUE)
  
  character_cols <- dictionaries$names[column_type == 'C']$column_name %>% intersect(colnames(dat))
  pums[, (character_cols) := purrr::map(.SD, as.character), .SDcols = character_cols]
  
  # remove the separate tables from memory
  rm(per)
  rm(hu)
  
  # 1. discard columns ----
  
  # weight columns
  pums <- discard_cols(dt = pums, cols = c('^WGTP', '^PWGTP'), regex = TRUE)
  
  # flag columns
  flag_columns <- dictionaries$names[str_detect(description, 'allocation flag')]
  pums <- discard_cols(dt = pums, cols = flag_columns, regex = TRUE)
  
  # constant_columns
  unique_values <- data.table(column = colnames(pums),
                              unique_values = purrr::map_int(pums, uniqueN)
                              )[unique_values == 1]
  
  pums <- discard_cols(dt = pums, cols = unique_values$column)
  
  # 2. data imputation ----
  numeric_impute <- dictionaries$names[column_type == 'N']$column_name %>%
    intersect(colnames(pums)) %>% 
    setdiff('PINCP') # our target variable deserves special consideration 
  
  na_fill(dt = pums,  
          cols = numeric_impute,
          replacement = 0)
  
  # for categoricals, only the variables that have an explicit 'b' = 'Does not apply' field will be replaced.
  categorical_impute <- dictionaries$names[column_type == 'C']$column_name %>%
    intersect(colnames(pums)) %>% 
    intersect(dictionaries$values[column_type == 'C' & starting_value == 'b']$column_name)
  
  na_fill(dt = pums,  
          cols = categorical_impute,
          replacement = 'b')
  
  
  # 3. categorical variable decoding ----
  
  # select the categorical variables according to the dictionary
  columns_to_decode <- dictionaries$names[column_type %chin% 'C']$column_name 
  
  # do not decode range categorical variables (ie where starting_value != ending_value)
  # These are c("SERIALNO", "SERIALNO", "PUMA", "MIGPUMA", "POWPUMA", "RACNUM")
  exclude_from_decoding <- 
    dictionaries$values[column_type %chin% 'C' & starting_value != ending_value]$column_name
  
  columns_to_decode %<>% setdiff(exclude_from_decoding) 
  
  # and remove the columns we already removed from the dataaset
  columns_to_decode %<>% intersect(colnames(pums))
  
  # set all categorical variables as character vectors to avoid type inconsistencies 
  pums[, (columns_to_decode) := purrr::map(.SD, as.character), .SDcols = columns_to_decode]
  
  columns_to_decode %>% walk(decode_categoricals, dat = pums)
  
  tictoc::toc()
  
  return(pums[])
}

# create a new column marking the value band of each observation wrt a continuous column 
discretize_column <- function(dt,
                              amount_column,
                              band_cuts,
                              bands_column_name) {
  
  amount_column %<>% as.character()
  
  # create empty column
  data.table::setDT(dt)
  dt[, (bands_column_name) := NA_character_]
  
  # build index for the bands
  run <- seq_along(band_cuts)
  band_length <- length(band_cuts)
  
  # create names for the bands
  band_names <- paste0(band_cuts[-band_length], '-', band_cuts[-1])
  
  # assign band to each value
  purrr::walk(run, ~data.table::set(dt,
                                    which(band_cuts[.x] <= dt[[amount_column]] &
                                            dt[[amount_column]] <= band_cuts[.x + 1]),
                                    bands_column_name,
                                    band_names[.x])[])
  
  dt[, (bands_column_name) := factor(x = get(bands_column_name), levels = band_names)]
}

```

This document is submitted to Acxiom on behalf of Growth Acceleration Partners for your consideration.

# Problem Statement

The objective set is to predict the individual income (PINCP) of a Texas resident given a sample of the American Community Survey.

The project specifications include two datasets, a data dictionary, and a reference handbook split into 4 chapters and an appendix.

# Data Overview

Both datasets provided are part of the Public Use Microdata Sample (PUMS) initiative by the American Community Survey Office. This initiative is meant to give public access to most questions and answers on the American Community Survey (ACS), while safeguarding the confidentiality of its respondents. Each file is a sample of 1% of its corresponding ACS dataset, which means that larger margins of error should be expected than on ACS-provided summaries.

## Geographic data

The PUMS files contain, in hierarchical order: region, division, state and Public Use Microdata area (PUMA). The latter are non-overlapping areas that cover roughly 100,000 populations each, and this is the lowest level of disaggregation on the file, due to privacy considerations.

# Data processing

We visibly load the libraries that will be used in the study.

```{r libraries, warning=FALSE, message=FALSE}
library(chronicle) # Developed by Philippe Heymans, see its Appendix section for further details 
# remotes::install_github(repo = 'pheymanss/chronicle')
library(data.table)
library(knitr)
library(ggplot2)
library(magrittr)
library(purrr)
library(stringr)
library(readr)
library(skimr)
```

## Data dictionary parsing

To get a better understanding of the data, we proceed to map the PUMS variable names to their corresponding descriptions. The parsing of the data dictionary was rather troublesome, since the ACS houses two record types carelessly braided together on the same .csv file:

-   The specification for each field, which comprises 5 values:

1.  Record type (NAME or VAL).
2.  Column name.
3.  Column type (numeric or categorical).
4.  Field length.
5.  Description.

-   The specification for each possible field value, which comprises the same values, but additionally the range of values that each entry can take.

1.  Record type (NAME or VAL).
2.  Column name.
3.  Column type (numeric or categorical).
4.  Field length.
5.  Starting Value (first element of the value range).
6.  Ending Value (last element of the value range).
7.  Description.

This makes the VAL record type a more complete mapping of the content of the data, but the NAME record type is more prectical for quick checks. We parse them both separately into a list.

```{r dictionary_parsing}
dict_filename <- 'Technical Test Data and Information/PUMS_Data_Dictionary_2019.csv'

dictionaries <- readr::read_lines(dict_filename) %>% 
  unique() %>% 
  split(str_detect(., '^NAME')) %>% # identify each dataset
  purrr::set_names(c('values', 'names')) %>% 
  purrr::map2(.y = list(val_columns = c('field_type',
                                        'column_name',
                                        'column_type',
                                        'field_length',
                                        'starting_value',
                                        'ending_value',
                                        'description'),
                        name_columns = c('field_type',
                                         'column_name',
                                         'column_type',
                                         'field_length',
                                         'description')),
              .f = ~data.table::fread(text = c(paste(.y, collapse = ','),
                                               .x)))
```

Additionally, a mark was added to point if the variable belongs to the Person or the Housing Unit dataset. The first variable of the Person dataset is SPORDER, so any entry before it belongs to the Housing Unit data.

```{r dict_mark_h_or_p}
# find the index of SPORDER in both the $names and $values tables
sporder_index <- dictionaries %>% 
  purrr::map(~which(.x$column_name %chin% 'SPORDER'))

# map the corresponding dataset flag to each row
walk2(.x = dictionaries, 
      .y = sporder_index,
      .f = ~.x[, record_number := 1:.N
             ][, dataset := data.table::fifelse(test = record_number < .y,
                                           yes = 'H',
                                           no = 'P')]) 
```

With this code, this:

```{r, echo=FALSE}
readr::read_lines(dict_filename)[1:20] %>% paste('\n') %>%  cat()
```

Turns into these:

**Column dictionary**

```{r, echo=FALSE}
knitr::kable(head(dictionaries$names))
```

**Value dictionary**

```{r, echo=FALSE}
knitr::kable(head(dictionaries$values))
```

These description fields do not give details regarding methodology nor definitions, for the latter there are the [US Census Bureau subject definitions](https://www.census.gov/programs-surveys/cps/technical-documentation/subject-definitions.html).


And for future convenience, this process can be packaged into a single call:

```{r}
dict_filename <- 'Technical Test Data and Information/PUMS_Data_Dictionary_2019.csv'
dicts <- parse_dictionaries(dict_filename)

str(object = dicts, give.atr = F, level = 2, vec.len = 2)

```

## Data Loading and joining

At 253MB (70MB + 183MB), this dataset should not present issues on any reasonably modern hardware setup, and that is precisely the reason why the PUMS dataset exists. Since the PUMS files are a 1% random sampling of the full state dataset, and assuming reasonable stability in the size of the fields, the size of the full dataset would range around 25GB. This would indubitably be quite more demanding, but still not enough to be a limiting factor of the steps carried out in this particular study. 

If this was a national study done with the full ACS dataset instead of a Texas-focused PUMS study, then the size of the raw dataset would indeed be a limiting factor, and loading the entire dataset into memory would not be feasible on consumer-grade hardware. Supposing that it is not worth it to load the dataset into a distributed filesystem setup, the healthier approach would be to start discarding portions of the data that would not be used in the model. If discarding all those columns was not enough for the full dataset, then performing a sample of the data could suffice to diagnose, re-structure and calibrate the desired model. However, assuring a representative sampling can potentially be more time consuming than the study itself.

Feature selection will be performed further on, but these will be motivated by good statistical practices, not by data size problems. 

We will load both datasets into memory. The column 'RT' is discarded because it is an indicator of the dataset, P for Person and H for Housing Unit, wich will become meaningless once we merge both files.

```{r load_data}
# load person data
per <- data.table::fread('Technical Test Data and Information/psam_p48.csv') %>% 
  discard_cols(cols = 'RT')
# 272,776 rows

# load housing unit data
hu <- data.table::fread('Technical Test Data and Information/psam_h48.csv') %>% 
  discard_cols(cols = 'RT')
# 123,710 rows

```

Since the amount of columns of each table is quite extensive, we will first join both tables, and seek to keep only the variables that both are potentially relevant to the study at hand, and have a reasonable amount of valid data. Although the documentation suggests joining the tables by 'SERIALNO', the records also share several location variables, which will be also used on the join criteria to avoid duplication.

```{r merge_data}
join_cols <- intersect(colnames(per), colnames(hu))
# this is c("SERIALNO", "DIVISION", "PUMA", "REGION", "ST", "ADJINC")

# set data.table keys for fast joining
data.table::setkeyv(per, join_cols)
data.table::setkeyv(hu,  join_cols)

# data.table left join
dat <- data.table::merge.data.table(x = per, hu, all.x = TRUE)

# remove the separate tables from memory
rm(per)
rm(hu)

# map all character variables as.character:
# first, get all the column names in the dictionary that correspond to 'C' column types. 
character_cols <- dictionaries$names[column_type == 'C']$column_name %>% intersect(colnames(dat))

# and then coerce all of them to character by reference
dat[, (character_cols) := purrr::map(.SD, as.character), .SDcols = character_cols]
```

## Column discarding

The joint table comprehends 517 variables, but we can quickly reduce the width of the dataset with three simple ideas:

-   Over 100 of those columns are weight values for computing representative aggregates, which is not part of the scope of this study.

```{r remove_wieght_columns}
dat <- discard_cols(dt = dat, cols = c('^WGTP', '^PWGTP'), regex = TRUE)
```

-   We will not discriminate on whether the item comes from a true response or an [allocated response](https://www.census.gov/programs-surveys/acs/methodology/sample-size-and-data-quality/item-allocation-rates-definitions.html).

```{r remove_flag_allocation}
flag_columns <- dictionaries$names[str_detect(description, 'allocation flag')]$column_name
dat <- discard_cols(dt = dat, cols = flag_columns, regex = TRUE)
```

-   We can remove columns that have a single constant value.

```{r remove_constant_columns}
unique_values <- data.table(column = colnames(dat),
                            unique_values = purrr::map_int(dat, uniqueN)
                            )[unique_values == 1
                            ][, description := describe_column(column)][]

knitr::kable(unique_values)
```

As expected, these columns mostly correspond to geographical entries that are all the same for all Texan residents, and adjustment coefficients that are all the same for every observation done on the same time period.

```{r remove_constant_columns2}
dat <- discard_cols(dt = dat, cols = unique_values$column)
```

With that, we went from 517 to 218 variables, a much more manageable amount of columns to consider in our model. We can now proceed with studying the contents of such columns.

## Valid data coverage

Let us first examining the percentage of valid data of all the other variables.

```{r}
na_perc <- na_percentage(dat) %>% 
  data.table::merge.data.table(y = dictionaries$names[, .(column_name, column_type, description)],
                               by.x = 'column',
                               by.y = 'column_name')

na_perc[order(-na_perc)] %>% head(15) %>% knitr::kable()

```

Most variables with high NA coverage are ones for which there exists some sort of "No data/Does not apply" value, but it is not explicitly stated, instead it is left empty (which is a reasonable space saving decision) and subsequently read as an NA by R. This means that there is no need to discard most of these variables, since they are easily mapped into the default response 'b' == 'Does not apply 'for categorical variables, and 0 would be appropriate for most numerical variables since those represent costs, payments and counts.

```{r data_imputation}
numeric_impute <- dictionaries$names[column_type == 'N']$column_name %>%
  intersect(colnames(dat)) %>% 
  setdiff('PINCP') # our target variable deserves special consideration 

na_fill(dt = dat,  
        cols = numeric_impute,
        replacement = 0)

# for categoricals, only the variables that have an explicit 'b' = 'Does not apply' field will be replaced.
categorical_impute <- dictionaries$names[column_type == 'C']$column_name %>%
  intersect(colnames(dat)) %>% 
  intersect(dictionaries$values[column_type == 'C' & starting_value == 'b']$column_name)

na_fill(dt = dat,  
        cols = categorical_impute,
        replacement = 'b')

```

Recomputing the valid data coverage metrics, we see much better numbers for most variables, only keeping NAs in the 9 categorical variables that are not mappable to a 'b' field, and our target variable that we explicitly decided not to impute.

```{r na_perc_cleansed, echo=TRUE}
na_perc <- na_percentage(dat) %>% 
  data.table::merge.data.table(y = dictionaries$names[, .(column_name, column_type, description)],
                               by.x = 'column',
                               by.y = 'column_name')

na_perc[na_perc > 0] %>% head(10) %>% knitr::kable()

```

## Categorical variable decoding

Now, with all most variables filled, we can proceed and map them into their true values. The strategy here is to fetch the values specified in the data dictionary, and rewrite the values of each coded column by reference for the minimum memory overhead possible. We first create the list of categorical variables to be decoded:

```{r pre_decoding}
# select the 'C' variables according to the dictionary
columns_to_decode <- dictionaries$names[column_type %chin% 'C']$column_name 

# do not decode range categorical variables (ie where starting_value != ending_value)
# These are c("SERIALNO", "SPORDER", "PUMA", "MIGPUMA", "POWPUMA", "RACNUM")
exclude_from_decoding <- 
  dictionaries$values[column_type %chin% 'C' & starting_value != ending_value]$column_name

columns_to_decode %<>% setdiff(exclude_from_decoding) 

# and of course, decode only the columns that are still present in the data
columns_to_decode %<>% intersect(colnames(dat))
```

The following function then rewrites the content of all coded columns into their explicit values. For example, for the 'AGS' column (Sales of Agriculture Products): the value '1' will be changed to 'None', '2' will be changed to '\$1 - \$999', and so on.

```{r, variable_decoding}
# and define a function that decodes a column
decode_categoricals <- function(col, dat){
  # select the dictionary entries speciying column values
  column_mapping <- dictionaries$values[column_name %chin% col & starting_value == ending_value,
                                        .(starting_value, description)] %>% 
    data.table::setkey(starting_value)
  
  # data.table 'left join' by reference
  # technically this is a mutate by reference, not a join
  matching_statement <- paste0(col, '==', 'starting_value')
  setkeyv(dat, col)
  dat[column_mapping, on = matching_statement, (col) := i.description]
}

tictoc::tic() 
# apply the function to all categorical columns
columns_to_decode %>% walk(decode_categoricals, dat = dat)
tictoc::toc()
```

On this execution time, the function was able to compute the mappings of all 169 coded columns, rewriting over the same pointers which also means a very minimal memory overhead.

With this final step, not only do we have a workable data file that needs no external reference (besides column names, whose descriptions are too long to use as column names directly), but this also can be wrapped into into a single function, allowing us to efficiently parse all past and future PUMS files that follow the same structure.

```{r full_preprocess_function, warning=FALSE, message=FALSE}
per_fn <- 'Technical Test Data and Information/psam_p48.csv'
hu_fc <- 'Technical Test Data and Information/psam_h48.csv'
dict_filename <- 'Technical Test Data and Information/PUMS_Data_Dictionary_2019.csv'

pums <- pums_for_individual_modelling(person_filename = per_fn,
                                      housing_unit_filename = hu_fc,
                                      dictionary_filename = dict_filename)
```

# Exploratory data analysis


## Target variable analysis

As we have an explicit target variable, we can firts center our data exploration in function of the relationship between each variable and our y variable: PINCP. This is a continuous numerical variable, which means we are dealing with a regression problem. We start off by understanding our variable:

```{r income_num_summ}
num_summary(dt = dat, col = 'PINCP') %>% knitr::kable()
```

From these statistics, we can see that the variable reaches negative values, which is a valid data entry if the respondent experienced losses in the study period. There is no no solid reason currently to discard those records.

On the other hand, and as previously pointed out in the prior section, 18% of the PINCP fields are empty, and there are records with 0 assigned to that column, hence it would be reckless to conclude that missing data means zero. We will remove those observations from the data.

```{r remove_income_nas}
dat <- dat[!is.na(PINCP)]
```

We reach then 222,893 valid observations of an individual.

And now we can proceed with visualizing our variable. As expected with income variables, its distribution is heavily asymmetrical, heavily right skewed even after the variable being bounded at 1,166,600.

```{r PINCP_density, fig.width=8}
chronicle::make_density(dt = dat, value = 'PINCP', static = FALSE)
```

Then, we can study the relationship between PINCP and all other numerical variables through a linear correlation table

```{r correlations}
# select numeric columns only
nums <- dat %>% purrr::keep(is.numeric) %>% colnames()

# compute correlations against the individual ncome variable, and count NA values 
income_cor <- data.table(column_name = nums,
                  income_cor = purrr::map_dbl(.x = nums,
                                              .f = ~cor(na.omit(dat[, .(PINCP, get(.x))]))[1,2] %>% round(2)),
                  na_perc = purrr::map_dbl(.x = nums, 
                                           .f = ~mean(is.na(dat[[.x]])))
                  )[, description := describe_column(column_name)]

income_cor[order(-abs(income_cor))] %>% knitr::kable()
```

Because most numerical variables are likely to be higher on better-off individuals, it stands to reason that most of them are positively correlated with our income target variable.

We will however remove all components of the PINCP variable present in the training data (ie all [income categories](https://www.census.gov/programs-surveys/cps/technical-documentation/subject-definitions.html#incomemeasurement)) since those variables give out an explicit sum to get our target variable, and keeping them would defeat the purpose of the exercise.

```{r discard_wage_earnings_variables}
dat <- discard_cols(dt = dat, cols = c('PERNP', 'WAGP', 'OIP', 
                                       'PAP',  'RETP', 'INTP', 
                                       'SEMP', 'SSIP', 'SSP', 
                                       'WAGP', 'WKHP'))
```


### Consistency within housing unit income

It is also necessary to verify that the household income information is consistent with the income of each of its members. The fist check should be to verify that all members of the same housing unit report the same household income.

```{r check_p_h_income}

# set key for faster group statistics
data.table::setkey(dat, SERIALNO)

# query all housing units with more than 1 HINCP reported
dat[, .(hu_incomes_reported = uniqueN(HINCP)), by = .(SERIALNO)][hu_incomes_reported > 1]
```

This returns an empty query, which means that there is only one single value reported for each housing unit.

Next, we should verify that the individual income makes sense in the context of their housing unit. One might be inclined to verify that every individual's income is smaller than income of the the housing unit it resides, however as we saw in the exploration of our target variable, individual income can be negative, hence one or more individual's income can be larger than their household's income to account for other individual's losses.

The correct way to review this consistency then, is to verify that the sum of the individual incomes is equal to the housing income, including negative incomes 

```{r}
income_check <- dat[, .(sum_individual = sum(PINCP), HINCP = unique(HINCP)), 
                    keyby = SERIALNO
                  ][, consistent := sum_individual == HINCP]

income_check[, .N, consistent][, perc := N/sum(N)][] %>% knitr::kable()
```
We see that roughly 95% of the households have perfectly consistent income reports. For those that do not, we can review how large is the difference. But a quick glance at the summary lets us see that all the inconsistent data is due to imputed missing values, hence the only income value in all inconsistent  households is 0.

```{r}
head(income_check[consistent == FALSE]) %>% knitr::kable()
income_check[consistent == FALSE]$HINCP %>% unique()
```

We can then use the computed summary to correct these missing values with their true values

```{r}
dat[income_check, on = 'SERIALNO == SERIALNO', HINCP := i.sum_individual]
```
And repeating the same query, we see now that 100% of the housing unit income observations are consistent

```{r}
dat[, .(sum_individual = sum(PINCP), HINCP = unique(HINCP)), 
                    keyby = SERIALNO
  ][, consistent := sum_individual == HINCP
  ][, .N, consistent][, perc := N/sum(N)][] %>%  knitr::kable()
```


## Numerical variables

 We can now move our attention into the numerical variables generally. The skim() function from the {skimr} package lets us quickly skim through all numerical variables with the skim() function. Categorical variables are also supported by the function, but are omitted to avoid a very large table output.

```{r skim}
# create a copy of dat with a snippet of the variable description pasted into each column name
dat_with_descriptions <- data.table::copy(dat)
desc_index <- match(colnames(dat), dictionaries$names$column_name)
desc_colnames <- paste0(colnames(dat), ': ', str_sub(dictionaries$names$description[desc_index],1,30))
colnames(dat_with_descriptions) <- desc_colnames

skimr::skim(keep(dat_with_descriptions, is.numeric))
```

From this, we can see that most numerical variables are right skewed, which is consistent with the imputation process assigning them 0 on all their missing data fields. This, however, is also consistent with the interpretation of the variables, so it is not considered data distortion as much as it is data completion.

We also want to see the correlation between all variables. Due to the large amount of variables still present, we opt for an interactive heatmap to glance through the data.

```{r num_correlations, fig.width=8, fig.height=8}
cor_dat <- cor(keep(dat, is.numeric)) %>% 
  round(2) %>% 
  as.data.table(keep.rownames = TRUE) %>% 
  data.table::melt.data.table(id.vars = 'rn', measure.vars = colnames(.)[-1]) %>% 
  setnames(old = c('rn', 'variable'), new = c('x', 'y'))

cor_dat[, x := factor(x, levels = sort(unique(x)))
      ][, y := factor(y, levels = sort(unique(x), decreasing = TRUE))]


(ggplot(data = cor_dat, 
                 mapping = aes(x = x, y = y, fill = value)) +
    geom_tile() +
    theme_minimal() +
    theme(panel.background = element_rect(fill = "transparent", colour = NA),
                   plot.background =  element_rect(fill = "transparent", colour = NA), 
                   axis.title = element_blank(),
                   axis.text.x = element_blank()) +
    viridis::scale_fill_viridis(option = 'magma')) %>% 
  plotly::ggplotly()

```

## Categorical Variables

For categoricals, a good approach would be visualizing the densities of the target variable for each class, but since we have around hundreds of categorical variables, it is not feasible to do it on every single one in the given time frame. We manually select a few variables that seem sensible to single out. The selected variables are:

```{r categorical_tables, echo=FALSE}
categoricals_to_explore <- c("HINS4",
                             "MAR",
                             "WKL",
                             "ESR",
                             "SCIENGP",
                             "SCIENGRLP",
                             "FS",
                             "HISPEED",
                             "TEN",
                             "KIT",
                             "SMX") %>% 
  sort() %>% 
  purrr::set_names(.)

data.table(Variable = categoricals_to_explore 
         )[order(Variable)
         ][, Description := describe_column(Variable)] %>%
  kable()
```

We then proceed to make box plots of all the selected variables. Because PINCP has around 10,000 records (5% of all observations) above 150,000, we will truncate the charts at that value to still be able to easily grasp differences in the medians of the categorical groups, without the plot being overtaken by the higher and unusual values.

```{r, warning=FALSE, message=FALSE}
categorical_plots <- categoricals_to_explore %>% 
  purrr::map2(.x = categoricals_to_explore, 
              .y = describe_column(categoricals_to_explore), 
              .f = ~chronicle::make_boxplot(dt = dat[PINCP < 150000],
                                            value = 'PINCP', 
                                            groups = .x,
                                            jitter = FALSE, 
                                            x_axis_label = .y, 
                                            static = FALSE)) %>%
  set_names(categoricals_to_explore)
```

### Employment status recode

```{r, warning=FALSE, echo=FALSE}
categorical_plots$ESR
```

### Yearly food stamp/Supplemental Nutrition Assistance Program (SNAP) recipiency

```{r, warning=FALSE, echo=FALSE}
categorical_plots$FS
```

### Medicaid, Medical Assistance, or any kind of government-assistance plan for those with low incomes or a disability

```{r, warning=FALSE, echo=FALSE}
categorical_plots$HINS4
```

### Broadband (high speed) Internet service such as cable, fiber optic, or DSL service

```{r, warning=FALSE, echo=FALSE}
categorical_plots$HISPEED
```

### Complete kitchen facilities

```{r, warning=FALSE, echo=FALSE}
categorical_plots$KIT
```

### Marital status

```{r, warning=FALSE, echo=FALSE}
categorical_plots$MAR
```

### Field of Degree Science and Engineering Flag - NSF Definition

```{r, warning=FALSE, echo=FALSE}
categorical_plots$SCIENGP
```

### Field of Degree Science and Engineering Related Flag - NSF Definition

```{r, warning=FALSE, echo=FALSE}
categorical_plots$SCIENGRLP
```

### Second or junior mortgage or home equity loan status

```{r, warning=FALSE, echo=FALSE}
categorical_plots$SMX
```

### Tenure

```{r, warning=FALSE, echo=FALSE}
categorical_plots$TEN
```

### When last worked

```{r, warning=FALSE, echo=FALSE}
categorical_plots$WKL
```

As expected, most of these variables give a clear quantile/median distinction in regards to the individual income. These variables were chosen because they are proxies for poverty/quality of life, and have few distinct values to be comfortably visualized.

# Feature Selection

A good feature selection process should include both subject-matter expertise and sensible statistical behaviors. It also should take into account the use of the model, and whether or not it will help perpetuate discrimination over protected groups like ethnicity, gender, nationality, migratory status, religious belief, disability, and so on. Since the purpose of the model was not disclosed as part of the briefing, this type of variables will not be taken into account on the study, and will be explicitly removed from the modeling dataset:

```{r, echo=FALSE}
protected_categories <- c("CIT",
                          "CITWP",
                          "DREM",
                          "HINS7",
                          "SEX",
                          "DIS",
                          "HISP",
                          "NATIVITY",
                          "ANC1P",
                          "ANC2P", 
                          "RAC1P",
                          "RAC2P",
                          "RAC3P",
                          "RACAIAN",
                          "RACASN",
                          "RACBLK",
                          "RACNH",
                          "RACNUM",
                          "RACPI",
                          "RACSOR",
                          "RACWHT",
                          "CPLT",
                          "FES")
data.table(protected_categories, description = describe_column(protected_categories)) %>% knitr::kable()
```

```{r}
dat <- discard_cols(dat, protected_categories)
```

We can also build hand-picked variables, which are self-explanatory. We add a count on the NFP to count the respondents themselves along their family members.

```{r, warning=FALSE, message=FALSE }
dat[, hu_per_capita_income := HINCP/NP
  ][, fam_per_capita_income := FINCP/(NPF+1)
  ][, fam_children_percentage := fifelse(test = AGEP < 18, 
                                     yes = (NRC+1)/(NPF+1),
                                     no = (NRC)/(NPF+1))
  ][, rooms_per_person := RMSP/NP 
  ][, bedrooms_per_person := BDSP/NP 
  ][, commute_as_perc_worked := JWMNP/WKWN]
```

Since we are confident about our data cleansing process, and good data practices from the Census Bureau, we can rely on H2O's randomForest heuristics to drop 'bad columns'. The main reason why a variable might be dropped is it being a categorical variable with a large quantity of distinct values, since ensemble tree models tend to memorize the categories and end up overfitting the data even in a small number of epochs. We will not make outlier treatments since the Bureau already censors extreme values to keep the privacy of the respondents, and since we are going to use a tree based model, feature scaling is not needed.


# Modelling

## H2O local cluster setup

We first setup a local H2O cluster, allocate it 16GB of RAM and split the data into a train and testing set. The training will be done with cross validation, but we stil want to test its performance on unseen data.


```{r h2o_setup, warning=FALSE, message=FALSE}
library(h2o)

#set up the H2O local cluster
h2o.init(max_mem_size = '16G') # allocating 16GB of RAM
h2o.no_progress() # disable progress bars

# feed all variables to the model (except indexes and the target variable)
x <- setdiff(colnames(dat), c('PINCP', 'SERIALNO', 'SPORDER'))
y <- 'PINCP'

# import dataset into h2o cluster
model_dat <- as.h2o(dat)

dat_split <- h2o.splitFrame(data = model_dat, ratios = 0.8, seed = 1)
train <- dat_split[[1]]
test <- dat_split[[2]]
```

Then we start off with a random forest model with H2O's very reasonable preset values (ntrees = 50, max_depth = 20, nbins = 20, no early stopping)

## Initial model

```{r rf, message=FALSE, warning=FALSE}
# build a cross validated model
rf <- h2o.randomForest(x = x,
                       y = y,
                       training_frame = train,
                       nfolds = 10, 
                       max_runtime_secs = 100, 
                       seed = 1)
```

```{r}
rf@model$model_summary %>% knitr::kable()
```

And we can see there is stability in the cross validation metrics

```{r}
rf@model$cross_validation_metrics_summary %>% knitr::kable()
```

And we can see that the performance on the unseen data is virtually identical to the cross validation metrics. The error metrics seem to be high, however, especially MAE and RSME in context of the income bands we saw through our variable exploration.

```{r rf1_model_performance}
# check performance over test set
rf@model$cross_validation_metrics
h2o.performance(model = rf, newdata = test)
```

## Second model

We will try to improve the model by letting it fit the data with deeper trees and more run time.

```{r, warning=FALSE, message=FALSE}
rf <- h2o.randomForest(x = x,
                       y = y,
                       training_frame = train,
                       nfolds = 10, 
                       max_runtime_secs = 600, 
                       seed = 1, 
                       max_depth = 30)

rf@model$model_summary %>% knitr::kable()
```

```{r rf2_model_performance}
# check performance over test set
rf@model$cross_validation_metrics
h2o.performance(model = rf, newdata = test)
```
This does not yield significant improvements, and the scoring history suggests that the performance may not improve 
without extensive training time, time that is not available for this delivery.

```{r}
rf@model$scoring_history %>% knitr::kable()
```

## Brute forcing throur H2O's AutoML

We can then review if the limiting factor is the models/training time available by letting H2O's autoML function seek for an optimal model

```{r aml, warning=FALSE, message=FALSE}
aml <- h2o.automl(x = x,
                  y = y,
                  training_frame = train,
                  nfolds = 10, 
                  max_runtime_secs = 600, 
                  seed = 1)
aml@leaderboard %>% knitr::kable()
```

```{r}
aml@leader@model$model_summary
```

While the model complexity skyrockets, the performance metrics do not show an improvement meaningful enough to justify its additional complexity. We can then conclude that to improve the model, we would need to improve the feature engineering process, or reincorporate some income variables to use as proxies for our final goal. 

We then proceed finishing the model performance analysis with our random forest. 

## Contextualizing the error metrics

While MAE and RMSE give an idea of the range of values that might be predicted, they are not ideal to understand the fit for different income bands, since being 17,000 USD off means quite different things for people in the first vs last quintiles. Hence we propose using the median relative error to see the overall performance across different income bands.

```{r prediction}
# predict over the test set
pred <- h2o.predict(object = rf, 
                    newdata = test)

# compute relative error for each observation
pred_frame <- cbind(as.data.table(test)[, .(PINCP)],
                    as.data.table(pred)
                    )[, absolute_error := abs(predict-PINCP)
                    ][, relative_error := absolute_error/PINCP]

# create a grouping column for income quintiles
discretize_column(dt = pred_frame,
                  amount_column = 'PINCP',
                  band_cuts = quantile(pred_frame$PINCP, probs = seq(0, 1, .2)), 
                  bands_column_name = 'income_band')

# check the performance in each quintile
pred_frame[, .(population_size = .N,
               median_absolute_error = median(absolute_error),
               median_relative_error = median(relative_error)),
           by = income_band][order(income_band)] %>% knitr::kable()
```
The model is seeing modestly good results in the upper income bands, but does quite poorly in the lower ones. It is visible that the model had smaller absolute errors on the lower bands, but not enough to overcome the higher penalty that the smaller bands entail in regards to relative error.

And to see the performance without evening out good and bad results, we can draw a line in the sand by defining a 'good' result as a predicted value with a relative error below .15, and see the percentage of good results in each income band.

```{r}
# flag good predictions
pred_frame[, good_prediction := relative_error < .15]

pred_frame[, .(perc_good_pred = mean(good_prediction)), by = income_band][order(income_band)] %>% knitr::kable()
```

In conclusion, our model is modestly competent and stable across the higher positive income bands, while taking a significant hit on the lower income bands.

To finalize, we proceed to revise which variables the model took into account. We do this by reviewing the model's [variable importance](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html#:~:text=0%20and%201.-,Variable%20Importance%20Calculation%20(GBM%20%26%20DRF),(decreased)%20as%20a%20result.) ranking.

```{r}
var_importance <- as.data.table(h2o.varimp(rf))[, cumulative_perc := cumsum(percentage)
                                              ][, relative_importance := NULL
                                              ][, description := describe_column(variable)]

head(var_importance, 10) %>% knitr::kable()
```

The model relied heavily in the household and family income, and their respective per-capita approximations. These were expected to be our most robust proxies for the personal income. Then hours worked, age and property value, which stands to reason that they all will positively correlate with individual income. 

# Closing Thoughts

Overall, we have obtained an interesting baseline for individual income prediction, and with the ACS richness in data, there is a lot of potential to improve upon these current findings. Some suggestions:

*  Cross worked hours with average salaries by occupation to distinguish the value of those hours.
*  Perform more complex household metrics like the count of different profiles by age, occupation, education level, and so on. For example, the count and percentage of college educated adults by household.
*  Use  time of arrival and departure to distinguish night shifts, part-time and freelance jobs. 
*  Extend training time from minutes to hours.
*  Use grid search methods for finding optimal forest parameters.
*  Train several models to predict each income separately, and then add their predictions.

# Appendix

## The {chronicle} package

This package was developed by Philippe Heymans, and aims to provide a seamless experience while building sophisticated and interactive visualizations in R, handling most of their challenges though its collection of helper functions. For further detail, please visit its [github page](https://github.com/pheymanss/chronicle).

## Helper functions

This is a small collection of facilitator functions meant to make the output more readable, and avoid distractions from formatting or complex code. With very little effort, these could be built into an R package and have one-liner custom solutions for PUMS data processing.

```{r helper_functions, eval=FALSE}

# fetch the column description from the NAME entries of the data dictionary
describe_column <- function(col = NULL){
  descrip_index <- match(col, dictionaries$names$column_name)
  descr <- dictionaries$names$description[descrip_index]
  
  if(any(is.na(descr))){
    descr[is.na(descr)] <- 'Not on ACS dictionary'
  }
  return(descr)
}

# run summary statistics over a numerical column of a data.table (optionally grouped)
num_summary <- function(dt, col, by = NULL){
  # rename the column to a generic variable (to avoid eval() or get() calls)
  data.table::setnames(x = dt, old = col, new = 'value')
  
  # perform the computations
  summ <- dt[, .(count = sum(!is.na(value)),
               distinct_values = uniqueN(value),
               na_count = sum(is.na(value)),
               na_perc = sum(is.na(value))/length(value),
               sum = sum(value, na.rm = TRUE),
               mean = mean(value, na.rm = TRUE),
               sd = sd(value, na.rm = TRUE),
               min = min(value, na.rm = TRUE),
               percentile.25 = quantile(value, 0.25, na.rm = TRUE),
               percentile.50 = quantile(value, 0.5, na.rm = TRUE),
               percentile.75 = quantile(value, 0.75, na.rm = TRUE),
               percentile.975 = quantile(value, 0.975, na.rm = TRUE),
               percentile.99 = quantile(value, 0.99, na.rm = TRUE),
               max = max(value, na.rm = TRUE)),
             by = by]
  
  # rename the column back to its original name
  data.table::setnames(x = dt, old = 'value', new = col)
  
  return(summ)
}

# compute the count and percentage of missing entries for each column of a data.frame 
na_percentage <- function(dt){
  data.table(column = colnames(dt),
             na_count = purrr::map_int(.x = dt, 
                                       .f = ~sum(is.na(.x))
                                       ))[, na_perc := na_count/nrow(dt)][]
}

# removes all columns matching a character vector (optionally regular expressions)
discard_cols <- function(dt, cols, regex = FALSE){
  if(!regex){
    dt %>% purrr::discard(colnames(.) %in% cols)
  }else{
    dt %>% purrr::discard(str_detect(colnames(.), paste(cols, collapse = '|')))
  }
}


na_fill = function(dt, cols, replacement = 0) {
  # match the indexes of the columns specified by the user
  index <- match(cols, colnames(dt))
  purrr::walk(index, ~data.table::set(dt, which(is.na(dt[[.x]])), .x, replacement))
}


# parse the two separate specifications entangled in the ACS dictionary file
parse_dictionaries <- function(dict_filename){
  dictionaries <- readr::read_lines(dict_filename) %>%
    unique() %>%
    split(stringr::str_detect(., '^NAME')) %>% # identify each dataset
    purrr::set_names(c('values', 'names')) %>%
    purrr::map2(.y = list(val_columns = c('field_type',
                                          'column_name',
                                          'column_type',
                                          'field_length',
                                          'starting_value',
                                          'ending_value',
                                          'description'),
                          name_columns = c('field_type',
                                           'column_name',
                                           'column_type',
                                           'field_length',
                                           'description')),
                .f = ~data.table::fread(text = c(paste(.y, collapse = ','),
                                                 .x)))
  
  # find the index of SPORDER in both the $names and $values tables
  sporder_index <- dictionaries %>%
    purrr::map(~which(.x$column_name %chin% 'SPORDER'))
  
  # map the corresponding dataset flag to each row
  walk2(.x = dictionaries,
        .y = sporder_index,
        .f = ~.x[, record_number := 1:.N
        ][, dataset := data.table::fifelse(test = record_number < .y,
                                           yes = 'H',
                                           no = 'P')])
  return(dictionaries)
}

# decode categorical columns with the values provided in the VAL dictionary.
decode_categoricals <- function(col, dat){
  # select the dictionary entries speciying column values
  column_mapping <- dictionaries$values[column_name %chin% col & starting_value == ending_value,
                                        .(starting_value, description)] %>% 
    data.table::setkey(starting_value)
  
  # data.table 'left join' by reference
  # technically this is a mutate by reference, not a join
  matching_statement <- paste0(col, '==', 'starting_value')
  setkeyv(dat, col)
  dat[column_mapping, on = matching_statement, (col) := i.description]
}

pums_for_individual_modelling <- function(person_filename,
                                          housing_unit_filename,
                                          dictionary_filename){

  tictoc::tic('PUMS files successfully parsed')
  # 0. load and join data ---
    
  # load person data
  per <- data.table::fread('Technical Test Data and Information/psam_p48.csv') %>% 
    discard_cols(cols = 'RT')

  # load housing unit data
  hu <- data.table::fread('Technical Test Data and Information/psam_h48.csv') %>% 
    discard_cols(cols = 'RT')

  dictionaries <- parse_dictionaries(dict_filename)
  
  join_cols <- intersect(colnames(per), colnames(hu))
  # this is c("SERIALNO", "DIVISION", "PUMA", "REGION", "ST", "ADJINC")
  
  # set data.table keys for fast joining
  data.table::setkeyv(per, join_cols)
  data.table::setkeyv(hu,  join_cols)
  
  # data.table left join
  pums <- data.table::merge.data.table(x = per, hu, all.x = TRUE)
  
  character_cols <- dictionaries$names[column_type == 'C']$column_name %>% intersect(colnames(dat))
  pums[, (character_cols) := purrr::map(.SD, as.character), .SDcols = character_cols]
  
  # remove the separate tables from memory
  rm(per)
  rm(hu)
  
  # 1. discard columns ----
  
  # weight columns
  pums <- discard_cols(dt = pums, cols = c('^WGTP', '^PWGTP'), regex = TRUE)
  
  # flag columns
  flag_columns <- dictionaries$names[str_detect(description, 'allocation flag')]
  pums <- discard_cols(dt = pums, cols = flag_columns, regex = TRUE)
  
  # constant_columns
  unique_values <- data.table(column = colnames(pums),
                              unique_values = purrr::map_int(pums, uniqueN)
                              )[unique_values == 1]
  
  pums <- discard_cols(dt = pums, cols = unique_values$column)
  
  # 2. data imputation ----
  numeric_impute <- dictionaries$names[column_type == 'N']$column_name %>%
    intersect(colnames(pums)) %>% 
    setdiff('PINCP') # our target variable deserves special consideration 
  
  na_fill(dt = pums,  
          cols = numeric_impute,
          replacement = 0)
  
  # for categoricals, only the variables that have an explicit 'b' = 'Does not apply' field will be replaced.
  categorical_impute <- dictionaries$names[column_type == 'C']$column_name %>%
    intersect(colnames(pums)) %>% 
    intersect(dictionaries$values[column_type == 'C' & starting_value == 'b']$column_name)
  
  na_fill(dt = pums,  
          cols = categorical_impute,
          replacement = 'b')
  
  
  # 3. categorical variable decoding ----
  
  # select the categorical variables according to the dictionary
  columns_to_decode <- dictionaries$names[column_type %chin% 'C']$column_name 
  
  # do not decode range categorical variables (ie where starting_value != ending_value)
  # These are c("SERIALNO", "SERIALNO", "PUMA", "MIGPUMA", "POWPUMA", "RACNUM")
  exclude_from_decoding <- 
    dictionaries$values[column_type %chin% 'C' & starting_value != ending_value]$column_name
  
  columns_to_decode %<>% setdiff(exclude_from_decoding) 
  
  # and remove the columns we already removed from the dataaset
  columns_to_decode %<>% intersect(colnames(pums))
  
  # set all categorical variables as character vectors to avoid type inconsistencies 
  pums[, (columns_to_decode) := purrr::map(.SD, as.character), .SDcols = columns_to_decode]
  
  columns_to_decode %>% walk(decode_categoricals, dat = pums)
  
  tictoc::toc()
  
  return(pums[])
}

# create a new column marking the value band of each observation wrt a continuous column 
discretize_column <- function(dt,
                              amount_column,
                              band_cuts,
                              bands_column_name) {
  
  amount_column %<>% as.character()
  
  # create empty column
  data.table::setDT(dt)
  dt[, (bands_column_name) := NA_character_]
  
  # build index for the bands
  run <- seq_along(band_cuts)
  band_length <- length(band_cuts)
  
  # create names for the bands
  band_names <- paste0(band_cuts[-band_length], '-', band_cuts[-1])
  
  # assign band to each value
  purrr::walk(run, ~data.table::set(dt,
                                    which(band_cuts[.x] <= dt[[amount_column]] &
                                            dt[[amount_column]] <= band_cuts[.x + 1]),
                                    bands_column_name,
                                    band_names[.x])[])
  
  dt[, (bands_column_name) := factor(x = get(bands_column_name), levels = band_names)]
}

```


## Session Info

```{r}
sessionInfo()
```
